{"cells":[{"metadata":{"_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"# %% [code]\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle\nfrom gensim.models import word2vec\nfrom sklearn.model_selection import StratifiedKFold\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\n\n# %% [code]\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\ncpu_device = torch.device(\"cpu\")\n\n# %% [code]\nclass Config():\n    \"\"\"配置参数\"\"\"\n    train_path = \"../input/word_train.csv\"   # 训练文件\n    test_path = \"../input/word_test.csv\"     # 测试文件\n    vocabulary_path = \"../input/word_vocabulary.pkl\"  # 词汇表文件\n    word2vec_path = \"../input/word_gensim-word2vec-200.model\"  # 词向量文件\n    \n    title_maxlen = 20   # title的最大词长度\n    content_maxlen = 400  # content的最大词长度\n    \n    EPOCH = 20         # 迭代次数\n    BATCH_SIZE = 64   # 批处理大小\n    n_splits = 5    # 交叉验证折数\n    \n    show_batch = 20  # 在整个训练中 每show_batch显示一次训练集和验证集的分数\n    model_path = \"model.ckpt\"  # 模型暂存路径\n    require_improvement = 10 * show_batch\n    \nconfig = Config()\n\n# %% [code]\ntrain = pd.read_csv(open(config.train_path, \"r\", encoding=\"utf-8\"))  # id, aid, rid, level, rtitle_words, rcontent_words, atitle_words, acontent_words\ntest = pd.read_csv(open(config.test_path, \"r\", encoding=\"utf-8\"))    # id, aid, rid, level, rtitle_words, rcontent_words, atitle_words, acontent_words\nvocabulary = pickle.load(open(config.vocabulary_path, \"rb\"))\nword2vec_model = word2vec.Word2Vec.load(config.word2vec_path)\n\n# %% [code]\ndef sequence2vector(sequence, maxlen):\n    \"\"\"将文本转为词语序\"\"\"\n    vec = np.zeros(maxlen)\n    words = str(sequence).split(\" \")\n    index = 0\n    for word in words[:maxlen]:\n        if word in vocabulary.keys():\n            vec[index] = vocabulary[word]\n            index += 1\n    return vec\n\ndef load_embedding_matrix():\n    \"\"\"生成词向量矩阵  (len(vocabulary) + 1, 200)\"\"\"\n    embedding_matrix = np.zeros((len(vocabulary) + 1, 200))\n    for word, index in vocabulary.items():\n        embedding_matrix[index] = word2vec_model[word]\n    return embedding_matrix\n\n# %% [code]\ntrain['rtitle_words'] = train['rtitle_words'].apply(lambda x : sequence2vector(x, config.title_maxlen))\ntrain['rcontent_words'] = train['rcontent_words'].apply(lambda x : sequence2vector(x, config.content_maxlen))\ntrain['atitle_words'] = train['atitle_words'].apply(lambda x : sequence2vector(x, config.title_maxlen))\ntrain['acontent_words'] = train['acontent_words'].apply(lambda x : sequence2vector(x, config.content_maxlen))\ntest['rtitle_words'] = test['rtitle_words'].apply(lambda x : sequence2vector(x, config.title_maxlen))\ntest['rcontent_words'] = test['rcontent_words'].apply(lambda x : sequence2vector(x, config.content_maxlen))\ntest['atitle_words'] = test['atitle_words'].apply(lambda x : sequence2vector(x, config.title_maxlen))\ntest['acontent_words'] = test['acontent_words'].apply(lambda x : sequence2vector(x, config.content_maxlen))\n\n# %% [code]\nembedding_matrix = load_embedding_matrix()\n\n# %% [code]\ndef getBatch(batch_size, rtitles, rcontents, atitles, acontents, labels=None, shuffle=True):\n    \"\"\"批数据生成器\"\"\"\n    \n    # 打乱数据\n    if shuffle:\n        if labels is not None:\n            tmp = list(zip(rtitles, rcontents, atitles, acontents, labels))\n            random.shuffle(tmp)\n            rtitles[:], rcontents[:], atitles[:], acontents[:], labels[:] = zip(*tmp)\n        else:\n            tmp = list(zip(rtitles, rcontents, atitles, acontents))\n            random.shuffle(tmp)\n            rtitles[:], rcontents[:], atitles[:], acontents[:] = zip(*tmp)\n    \n    sindex = 0\n    eindex = batch_size\n    while eindex < len(rtitles):\n        batch1 = torch.LongTensor(rtitles[sindex: eindex])\n        batch2 = torch.LongTensor(rcontents[sindex: eindex])\n        batch3 = torch.LongTensor(atitles[sindex: eindex])\n        batch4 = torch.LongTensor(acontents[sindex: eindex])\n        batch5 = None\n        if labels is not None:\n            batch5 = torch.LongTensor(labels[sindex: eindex])\n    \n        temp = eindex\n        eindex = eindex + batch_size\n        sindex = temp\n        \n        if labels is not None:\n            yield (batch1, batch2, batch3, batch4, batch5)\n        else:\n            yield (batch1, batch2, batch3, batch4)\n    \n    if eindex >= len(rtitles):\n        batch1 = torch.LongTensor(rtitles[sindex: eindex])\n        batch2 = torch.LongTensor(rcontents[sindex: eindex])\n        batch3 = torch.LongTensor(atitles[sindex: eindex])\n        batch4 = torch.LongTensor(acontents[sindex: eindex])\n        batch5 = None\n        if labels is not None:\n            batch5 = torch.LongTensor(labels[sindex: eindex])\n            \n        if labels is not None:\n            yield (batch1, batch2, batch3, batch4, batch5)\n        else:\n            yield (batch1, batch2, batch3, batch4)\n\n# %% [code]\nclass CNNModel(nn.Module):\n    \n    def __init__(self, embedding_dim, output_size, kernel_dim=100, kernel_sizes=(3, 4, 5), dropout=0.5):\n        super(CNNModel,self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix))\n        self.fc1 = nn.Linear(embedding_dim * 2, 200)\n        self.fc2 = nn.Linear(200, output_size)\n\n    def forward(self, rtitles, rcontents, atitles, acontents):  # (bs, maxlen)\n        rtitles = self.embedding(rtitles)   #(bs, maxlen, embedding_dim)\n        rcontents = self.embedding(rcontents)\n        atitles = self.embedding(atitles)\n        acontents = self.embedding(acontents)\n        \n        rtitles = torch.mean(rtitles, 1)  #(bs, embedding_dim)\n        rcontents = torch.mean(rcontents, 1)\n        atitles = torch.mean(atitles, 1)\n        acontents = torch.mean(acontents, 1)\n        \n        titles = torch.mul(rtitles, atitles)  # (bs, embedding_dim)\n        contents = torch.mul(rcontents, acontents)\n        \n        x = torch.cat((titles, titles), 1)  # (bs, embedding_dim*2)  连接\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# %% [code]\ndef evaluate(model, rtitles, rcontents, atitles, acontents, labels):\n    \"\"\"在验证集上进行测试\"\"\"\n    predict_labels = []\n    true_labels = []\n    with torch.no_grad():\n        for i, batch in enumerate(getBatch(config.BATCH_SIZE, rtitles, rcontents, atitles, acontents, labels)):\n            outputs = model(batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device))\n            predic = torch.max(outputs.data, 1)[1]\n            predict_labels.extend(predic.to(cpu_device))\n            true_labels.extend(batch[4])\n    return metrics.accuracy_score(true_labels, predict_labels)\n\ndef predict(model, rtitles, rcontents, atitles, acontents):\n    \"\"\"预测测试集\"\"\"\n    pres = []\n    with torch.no_grad():\n        for i, batch in enumerate(getBatch(config.BATCH_SIZE, rtitles, rcontents, atitles, acontents, shuffle=False)):\n            outputs = model(batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device))\n            pres.extend(list(outputs.to(cpu_device).numpy()))\n    return np.array(pres)\n\n\ndef to_result(stack_test, data):\n    labels = np.argmax(stack_test, axis=1)\n    labels += 1\n    data['level'] = labels\n    data[['id', 'level']].to_csv(\"submit.csv\", index=None, encoding=\"utf-8\")\n\n# %% [code]\n\nstack_test = np.zeros((test.shape[0], 4))\nstack_train = np.zeros((train.shape[0], 4))\n\nsfolder = StratifiedKFold(n_splits=config.n_splits, random_state=0, shuffle=True)\nfor i, (tr, va) in enumerate(sfolder.split(train['rtitle_words'], train['level'])):\n    print('stack:%d/%d' % ((i + 1), config.n_splits))\n    \n    # 训练集\n    train_rtitles = [train['rtitle_words'][index] for index in tr]\n    train_rcontents = [train['rcontent_words'][index] for index in tr]\n    train_atitles = [train['atitle_words'][index] for index in tr]\n    train_acontents = [train['acontent_words'][index] for index in tr]\n    train_label = [train['level'][index] - 1 for index in tr]\n    \n    # 验证集\n    test_rtitles = [train['rtitle_words'][index] for index in va]\n    test_rcontents = [train['rcontent_words'][index] for index in va]\n    test_atitles = [train['atitle_words'][index] for index in va]\n    test_acontents = [train['acontent_words'][index] for index in va]\n    test_label = [train['level'][index] - 1 for index in va]\n    \n    \n    # 定义模型\n    model = CNNModel(200, 4)\n    model.to(device)\n    loss_function = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    total_batch = 0\n    val_best = 0  # 验证集最好分数\n    last_improve = 0  # 上一次有提升的total_batch\n    flag = False  # 早停的标志\n    \n    for epoch in range(1, 1 + config.EPOCH):   # 迭代\n        if flag == True:\n            break\n        for j, batch in enumerate(getBatch(config.BATCH_SIZE, train_rtitles, train_rcontents, train_atitles, train_acontents, train_label)):\n            total_batch += 1\n            \n            optimizer.zero_grad()\n            \n            # forward + backward + optimize\n            outputs = model(batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device))\n            loss = loss_function(outputs, batch[4].to(device))\n            loss.backward()\n            optimizer.step()\n            \n            if total_batch % config.show_batch == 0:\n                val_acc = evaluate(model, test_rtitles, test_rcontents, test_atitles, test_acontents, test_label)\n                print(\"val_acc : \" + str(val_acc)) \n                if val_acc > val_best:\n                    val_best = val_acc\n                    last_improve = total_batch\n                    # 暂存模型\n                    torch.save(model.state_dict(), config.model_path)\n                \n                if total_batch - last_improve >= config.require_improvement:\n                    print(\"No optimization for a long time, auto-stopping...\")\n                    flag = True\n                    break\n                \n    \"\"\"结束一折的训练做的事情\"\"\"\n    model.load_state_dict(torch.load(config.model_path))\n    stack_test += predict(model, list(test['rtitle_words']), list(test['rcontent_words']), list(test['atitle_words']), list(test['acontent_words']))\n    stack_train[va] += predict(model, test_rtitles, test_rcontents, test_atitles, test_acontents)\n    print('stack [{}/{}] 最好的分数：{}'.format(i+1, config.n_splits, val_best))\n    \n# 保存并生成结果\nstack_test /= config.n_splits\nto_result(stack_test, test)\nnp.save(\"stack_train.npy\", stack_train)\nnp.save(\"stack_test.npy\", stack_test)\nprint(\"数据保存成功！\")\n","execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}